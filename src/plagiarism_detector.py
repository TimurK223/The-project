from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize
import nltk
import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import PyPDF2
from dataclasses import dataclass
import warnings

warnings.filterwarnings("ignore")

# NLP –±–∏–±–ª–∏–æ—Ç–µ–∫–∏


class PlagiarismDetector:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –ø–ª–∞–≥–∏–∞—Ç–∞"""

    def __init__(self, min_similarity_threshold: float = 0.3, language: str = "english"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –ø–ª–∞–≥–∏–∞—Ç–∞

        Args:
            min_similarity_threshold: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –æ—Ç—á–µ—Ç–∞
            language: —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–æ–≤ ('english' –∏–ª–∏ 'russian')
        """
        self.documents: List[Document] = []
        self.min_threshold = min_similarity_threshold
        self.language = language

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._initialize_nlp_components()

    def _initialize_nlp_components(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        print("‚öôÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")

        try:
            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
            required_packages = ["punkt", "wordnet", "stopwords", "punkt_tab"]

            for package in required_packages:
                try:
                    nltk.data.find(
                        f"tokenizers/{package}" if package == "punkt_tab" else package
                    )
                except LookupError:
                    print(f"  üì• –ó–∞–≥—Ä—É–∑–∫–∞ {package}...")
                    nltk.download(package, quiet=True)

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤
            if self.language == "russian":
                # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º SnowballStemmer –≤–º–µ—Å—Ç–æ WordNetLemmatizer
                from nltk.stem import SnowballStemmer

                self.lemmatizer = SnowballStemmer("russian")
                # –ü—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
                self.stop_words = {
                    "–∏",
                    "–≤",
                    "–≤–æ",
                    "–Ω–µ",
                    "—á—Ç–æ",
                    "–æ–Ω",
                    "–Ω–∞",
                    "—è",
                    "—Å",
                    "—Å–æ",
                    "–∫–∞–∫",
                    "–∞",
                    "—Ç–æ",
                    "–≤—Å–µ",
                    "–æ–Ω–∞",
                    "—Ç–∞–∫",
                    "–µ–≥–æ",
                    "–Ω–æ",
                    "–¥–∞",
                    "—Ç—ã",
                    "–∫",
                    "—É",
                    "–∂–µ",
                    "–≤—ã",
                    "–∑–∞",
                    "–±—ã",
                    "–ø–æ",
                    "—Ç–æ–ª—å–∫–æ",
                    "–µ–µ",
                    "–º–Ω–µ",
                    "–±—ã–ª–æ",
                    "–≤–æ—Ç",
                    "–æ—Ç",
                    "–º–µ–Ω—è",
                    "–µ—â–µ",
                    "–Ω–µ—Ç",
                    "–æ",
                    "–∏–∑",
                    "–µ–º—É",
                    "—Ç–µ–ø–µ—Ä—å",
                    "–∫–æ–≥–¥–∞",
                    "–¥–∞–∂–µ",
                    "–Ω—É",
                    "–≤–¥—Ä—É–≥",
                    "–ª–∏",
                    "–µ—Å–ª–∏",
                    "—É–∂–µ",
                    "–∏–ª–∏",
                    "–Ω–∏",
                    "–±—ã—Ç—å",
                    "–±—ã–ª",
                    "–Ω–µ–≥–æ",
                    "–¥–æ",
                    "–≤–∞—Å",
                    "–Ω–∏–±—É–¥—å",
                    "–æ–ø—è—Ç—å",
                    "—É–∂",
                    "–≤–∞–º",
                    "–≤–µ–¥—å",
                    "—Ç–∞–º",
                    "–ø–æ—Ç–æ–º",
                    "—Å–µ–±—è",
                    "–Ω–∏—á–µ–≥–æ",
                    "–µ–π",
                    "–º–æ–∂–µ—Ç",
                    "–æ–Ω–∏",
                    "—Ç—É—Ç",
                    "–≥–¥–µ",
                    "–µ—Å—Ç—å",
                    "–Ω–∞–¥–æ",
                    "–Ω–µ–π",
                    "–¥–ª—è",
                    "–º—ã",
                    "—Ç–µ–±—è",
                    "–∏—Ö",
                    "—á–µ–º",
                    "–±—ã–ª–∞",
                    "—Å–∞–º",
                    "—á—Ç–æ–±",
                    "–±–µ–∑",
                    "–±—É–¥—Ç–æ",
                    "—á–µ–≥–æ",
                    "—Ä–∞–∑",
                    "—Ç–æ–∂–µ",
                    "—Å–µ–±–µ",
                    "–ø–æ–¥",
                    "–±—É–¥–µ—Ç",
                    "–∂",
                    "—Ç–æ–≥–¥–∞",
                    "–∫—Ç–æ",
                    "—ç—Ç–æ—Ç",
                    "—Ç–æ–≥–æ",
                    "–ø–æ—Ç–æ–º—É",
                    "—ç—Ç–æ–≥–æ",
                    "–∫–∞–∫–æ–π",
                    "—Å–æ–≤—Å–µ–º",
                    "–Ω–∏–º",
                    "–∑–¥–µ—Å—å",
                    "—ç—Ç–æ–º",
                    "–æ–¥–∏–Ω",
                    "–ø–æ—á—Ç–∏",
                    "–º–æ–π",
                    "—Ç–µ–º",
                    "—á—Ç–æ–±—ã",
                    "–Ω–µ–µ",
                    "—Å–µ–π—á–∞—Å",
                    "–±—ã–ª–∏",
                    "–∫—É–¥–∞",
                    "–∑–∞—á–µ–º",
                    "–≤—Å–µ—Ö",
                    "–Ω–∏–∫–æ–≥–¥–∞",
                    "–º–æ–∂–Ω–æ",
                    "–ø—Ä–∏",
                    "–Ω–∞–∫–æ–Ω–µ—Ü",
                    "–¥–≤–∞",
                    "–æ–±",
                    "–¥—Ä—É–≥–æ–π",
                    "—Ö–æ—Ç—å",
                    "–ø–æ—Å–ª–µ",
                    "–Ω–∞–¥",
                    "–±–æ–ª—å—à–µ",
                    "—Ç–æ—Ç",
                    "—á–µ—Ä–µ–∑",
                    "—ç—Ç–∏",
                    "–Ω–∞—Å",
                    "–ø—Ä–æ",
                    "–≤—Å–µ–≥–æ",
                    "–Ω–∏—Ö",
                    "–∫–∞–∫–∞—è",
                    "–º–Ω–æ–≥–æ",
                    "—Ä–∞–∑–≤–µ",
                    "—Ç—Ä–∏",
                    "—ç—Ç—É",
                    "–º–æ—è",
                    "–≤–ø—Ä–æ—á–µ–º",
                    "—Ö–æ—Ä–æ—à–æ",
                    "—Å–≤–æ—é",
                    "—ç—Ç–æ–π",
                    "–ø–µ—Ä–µ–¥",
                    "–∏–Ω–æ–≥–¥–∞",
                    "–ª—É—á—à–µ",
                    "—á—É—Ç—å",
                    "—Ç–æ–º",
                    "–Ω–µ–ª—å–∑—è",
                    "—Ç–∞–∫–æ–π",
                    "–∏–º",
                    "–±–æ–ª–µ–µ",
                    "–≤—Å–µ–≥–¥–∞",
                    "–∫–æ–Ω–µ—á–Ω–æ",
                    "–≤—Å—é",
                    "–º–µ–∂–¥—É",
                }
            else:
                # –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
                self.lemmatizer = WordNetLemmatizer()
                self.stop_words = set(stopwords.words("english"))

            print("‚úì NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {str(e)}")
            print("–ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É...")
            self.lemmatizer = None
            self.stop_words = set()

    @dataclass
    class Document:
        """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""

        filename: str
        content: str
        processed_content: str = ""
        file_type: str = ""

    def load_documents(self, folder_path: str) -> None:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏

        Args:
            folder_path: –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        """
        folder = Path(folder_path)
        if not folder.exists():
            raise FileNotFoundError(f"–ü–∞–ø–∫–∞ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        supported_extensions = {".txt", ".pdf"}

        for file_path in folder.iterdir():
            if file_path.suffix.lower() in supported_extensions:
                try:
                    if file_path.suffix.lower() == ".txt":
                        content = self._read_txt_file(file_path)
                        file_type = "txt"
                    elif file_path.suffix.lower() == ".pdf":
                        content = self._read_pdf_file(file_path)
                        file_type = "pdf"
                    else:
                        continue

                    doc = self.Document(
                        filename=file_path.name, content=content, file_type=file_type
                    )
                    self.documents.append(doc)
                    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω: {file_path.name} ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")

                except Exception as e:
                    print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file_path.name}: {str(e)}")

    def _read_txt_file(self, file_path: Path) -> str:
        """–ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                return file.read()
        except UnicodeDecodeError:
            # –ü–æ–ø—ã—Ç–∫–∞ —Å –¥—Ä—É–≥–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
            with open(file_path, "r", encoding="cp1251") as file:
                return file.read()

    def _read_pdf_file(self, file_path: Path) -> str:
        """–ß—Ç–µ–Ω–∏–µ PDF —Ñ–∞–π–ª–∞"""
        text = ""
        try:
            with open(file_path, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        except Exception as e:
            print(f"  ‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {str(e)}")
        return text

    def preprocess_text(self, text: str) -> str:
        """
        –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not text or not text.strip():
            return ""

        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()

        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ü–∏—Ñ—Ä
        if self.language == "russian":
            # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
            text = re.sub(r"[^–∞-—è—ë\s]", " ", text, flags=re.IGNORECASE)
        else:
            # –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            text = re.sub(r"[^a-z\s]", " ", text)

        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r"\s+", " ", text).strip()

        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–±–µ–∑ NLTK –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã)
        try:
            tokens = word_tokenize(text, language=self.language)
        except BaseException:
            # –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
            tokens = text.split()

        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
        processed_tokens = []
        for token in tokens:
            if token not in self.stop_words and len(token) > 2:
                if self.lemmatizer:
                    try:
                        if hasattr(self.lemmatizer, "lemmatize"):
                            lemma = self.lemmatizer.lemmatize(token)
                        else:
                            # –î–ª—è SnowballStemmer
                            lemma = self.lemmatizer.stem(token)
                        processed_tokens.append(lemma)
                    except BaseException:
                        processed_tokens.append(token)
                else:
                    processed_tokens.append(token)

        return " ".join(processed_tokens)

    def process_all_documents(self) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        print("\n‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        for i, doc in enumerate(self.documents):
            try:
                original_length = len(doc.content)
                doc.processed_content = self.preprocess_text(doc.content)
                processed_length = len(doc.processed_content.split())
                print(
                    f"  {i+1}. {doc.filename}: {original_length} —Å–∏–º–≤. ‚Üí {processed_length} —Å–ª–æ–≤"
                )
            except Exception as e:
                print(f"  ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {doc.filename}: {str(e)}")
                doc.processed_content = ""
        print("‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def cosine_similarity_method(self, text1: str, text2: str) -> float:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –º–µ—Ç–æ–¥–æ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å TF-IDF

        Args:
            text1: –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –≤—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç

        Returns:
            –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –æ—Ç 0 –¥–æ 1
        """
        if not text1.strip() or not text2.strip():
            return 0.0

        try:
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform([text1, text2])
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return float(similarity)
        except Exception as e:
            print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ cosine_similarity: {str(e)}")
            return 0.0

    def longest_common_subsequence(self, text1: str, text2: str) -> float:
        """
        –ü–æ–∏—Å–∫ —Å–∞–º–æ–π –¥–ª–∏–Ω–Ω–æ–π –æ–±—â–µ–π –ø–æ–¥–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

        Args:
            text1: –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –≤—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç

        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        words1 = text1.split()
        words2 = text2.split()

        if not words1 or not words2:
            return 0.0

        m, n = len(words1), len(words2)

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
        dp = [[0] * (n + 1) for _ in range(m + 1)]

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if words1[i - 1] == words2[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1] + 1
                else:
                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])

        lcs_length = dp[m][n]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ —Å–∞–º–æ–≥–æ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        min_length = min(m, n)
        if min_length == 0:
            return 0.0

        return lcs_length / min_length

    def ngram_similarity(self, text1: str, text2: str, n: int = 3) -> float:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º N-gram

        Args:
            text1: –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –≤—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç
            n: —Ä–∞–∑–º–µ—Ä N-gram

        Returns:
            –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ Jaccard –¥–ª—è N-gram
        """

        def get_ngrams(text, n):
            words = text.split()
            if len(words) < n:
                return set()
            ngrams = set()
            for i in range(len(words) - n + 1):
                ngram = " ".join(words[i: i + n])
                ngrams.add(ngram)
            return ngrams

        ngrams1 = get_ngrams(text1, n)
        ngrams2 = get_ngrams(text2, n)

        if not ngrams1 or not ngrams2:
            return 0.0

        intersection = len(ngrams1.intersection(ngrams2))
        union = len(ngrams1.union(ngrams2))

        return intersection / union if union > 0 else 0.0

    def calculate_similarity_matrix(self) -> Dict[str, np.ndarray]:
        """
        –†–∞—Å—á–µ—Ç –º–∞—Ç—Ä–∏—Ü —Å—Ö–æ–∂–µ—Å—Ç–∏ –≤—Å–µ–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–∞—Ç—Ä–∏—Ü–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
        """
        n = len(self.documents)
        if n == 0:
            return {}

        filenames = [doc.filename for doc in self.documents]

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü
        cosine_matrix = np.zeros((n, n))
        lcs_matrix = np.zeros((n, n))
        ngram_matrix = np.zeros((n, n))
        combined_matrix = np.zeros((n, n))

        print(f"\nüßÆ –†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è {n} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        for i in range(n):
            for j in range(i, n):
                text1 = self.documents[i].processed_content
                text2 = self.documents[j].processed_content

                # –†–∞—Å—á–µ—Ç –≤—Å–µ–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
                cosine_sim = self.cosine_similarity_method(text1, text2)
                lcs_sim = self.longest_common_subsequence(text1, text2)
                ngram_sim = self.ngram_similarity(text1, text2, n=3)

                # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (—Å—Ä–µ–¥–Ω–µ–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ)
                combined = 0.4 * cosine_sim + 0.3 * lcs_sim + 0.3 * ngram_sim

                cosine_matrix[i, j] = cosine_sim
                cosine_matrix[j, i] = cosine_sim

                lcs_matrix[i, j] = lcs_sim
                lcs_matrix[j, i] = lcs_sim

                ngram_matrix[i, j] = ngram_sim
                ngram_matrix[j, i] = ngram_sim

                combined_matrix[i, j] = combined
                combined_matrix[j, i] = combined

        print("‚úì –†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω")

        return {
            "cosine": cosine_matrix,
            "lcs": lcs_matrix,
            "ngram": ngram_matrix,
            "combined": combined_matrix,
            "filenames": filenames,
        }

    def visualize_results(self, similarity_matrices: Dict[str, np.ndarray]) -> None:
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≤–∏–¥–µ —Ç–µ–ø–ª–æ–≤—ã—Ö –∫–∞—Ä—Ç

        Args:
            similarity_matrices: —Å–ª–æ–≤–∞—Ä—å —Å –º–∞—Ç—Ä–∏—Ü–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        if not similarity_matrices:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return

        filenames = similarity_matrices["filenames"]
        if len(filenames) < 2:
            print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return

        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            axes = axes.flatten()

            methods = ["cosine", "lcs", "ngram", "combined"]
            titles = [
                "Cosine Similarity",
                "LCS Similarity",
                "N-gram Similarity",
                "Combined Similarity",
            ]

            for idx, (method, title) in enumerate(zip(methods, titles)):
                ax = axes[idx]
                matrix = similarity_matrices[method]

                # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã
                sns.heatmap(
                    matrix,
                    annot=True,
                    fmt=".2f",
                    cmap="RdYlGn_r",
                    square=True,
                    ax=ax,
                    xticklabels=filenames,
                    yticklabels=filenames,
                    cbar_kws={"label": "Similarity Score"},
                    vmin=0,
                    vmax=1,
                )

                ax.set_title(title, fontsize=14, fontweight="bold")
                ax.set_xlabel("Documents")
                ax.set_ylabel("Documents")
                ax.tick_params(axis="x", rotation=45)
                ax.tick_params(axis="y", rotation=0)

            plt.tight_layout()
            plt.savefig("similarity_matrix.png", dpi=300, bbox_inches="tight")
            print("‚úì –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ similarity_matrix.png")
            plt.show()

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")
            # –ü—Ä–æ—Å—Ç–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            self._text_visualization(similarity_matrices)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –≤ —Ñ–∞–π–ª
        self._save_matrices_to_csv(similarity_matrices)

    def _text_visualization(self, similarity_matrices: Dict[str, np.ndarray]) -> None:
        """–¢–µ–∫—Å—Ç–æ–≤–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        filenames = similarity_matrices["filenames"]
        combined_matrix = similarity_matrices["combined"]

        print("\nüìä –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ (—Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤–∏–¥):")
        print("-" * (len(filenames) * 10 + 10))

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        header = " " * 15
        for name in filenames:
            header += f"{name[:8]:>8} "
        print(header)
        print("-" * (len(filenames) * 10 + 10))

        # –î–∞–Ω–Ω—ã–µ
        for i, name in enumerate(filenames):
            row = f"{name[:12]:12} "
            for j in range(len(filenames)):
                row += f"{combined_matrix[i, j]:7.2f} "
            print(row)

    def _save_matrices_to_csv(self, similarity_matrices: Dict[str, np.ndarray]) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü —Å—Ö–æ–∂–µ—Å—Ç–∏ –≤ CSV —Ñ–∞–π–ª—ã"""
        filenames = similarity_matrices["filenames"]

        for method in ["cosine", "lcs", "ngram", "combined"]:
            matrix = similarity_matrices[method]
            df = pd.DataFrame(matrix, index=filenames, columns=filenames)
            filename = f"{method}_similarity_matrix.csv"
            df.to_csv(filename)
            print(f"‚úì –ú–∞—Ç—Ä–∏—Ü–∞ {method} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filename}")

    def generate_report(self, similarity_matrices: Dict[str, np.ndarray]) -> None:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–º –ø–ª–∞–≥–∏–∞—Ç–µ

        Args:
            similarity_matrices: —Å–ª–æ–≤–∞—Ä—å —Å –º–∞—Ç—Ä–∏—Ü–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        if not similarity_matrices:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á–µ—Ç–∞")
            return

        combined_matrix = similarity_matrices["combined"]
        filenames = similarity_matrices["filenames"]
        n = len(filenames)

        print("\n" + "=" * 60)
        print("üìä –û–¢–ß–ï–¢ –û –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–û–ú –ü–õ–ê–ì–ò–ê–¢–ï")
        print("=" * 60)

        potential_plagiarism = []

        for i in range(n):
            for j in range(i + 1, n):
                similarity = combined_matrix[i, j]
                if similarity >= self.min_threshold:
                    potential_plagiarism.append(
                        (
                            filenames[i],
                            filenames[j],
                            similarity,
                            similarity_matrices["cosine"][i, j],
                            similarity_matrices["lcs"][i, j],
                            similarity_matrices["ngram"][i, j],
                        )
                    )

        if potential_plagiarism:
            print(
                f"\n‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(potential_plagiarism)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –ø–ª–∞–≥–∏–∞—Ç–∞:"
            )
            print("-" * 100)
            print(
                f"{'–î–æ–∫—É–º–µ–Ω—Ç 1':<25} {'–î–æ–∫—É–º–µ–Ω—Ç 2':<25} {'–û–±—â–∞—è':<8} {'Cosine':<8} {'LCS':<8} {'N-gram':<8}"
            )
            print("-" * 100)

            for doc1, doc2, combined, cosine, lcs, ngram in sorted(
                potential_plagiarism, key=lambda x: x[2], reverse=True
            ):
                print(
                    f"{doc1:<25} {doc2:<25} {combined:.2%}    {cosine:.2%}    {lcs:.2%}    {ngram:.2%}"
                )
        else:
            print(
                f"\n‚úÖ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –ø–ª–∞–≥–∏–∞—Ç–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (–ø–æ—Ä–æ–≥: {self.min_threshold:.0%})"
            )

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "=" * 60)
        print("üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê")
        print("=" * 60)
        print(f"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {n}")
        print(f"–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: {self.min_threshold:.0%}")

        if n > 1:
            avg_similarity = np.mean(combined_matrix[np.triu_indices(n, k=1)])
            max_similarity = np.max(combined_matrix[np.triu_indices(n, k=1)])
            print(f"–°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {avg_similarity:.2%}")
            print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.2%}")

            # –°–∞–º—ã–µ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            if n > 1 and max_similarity > 0:
                print(f"\nüîç –°–∞–º—ã–µ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
                indices = np.where(combined_matrix == max_similarity)
                for i, j in zip(indices[0], indices[1]):
                    if i < j:
                        print(f"  {filenames[i]} ‚Üî {filenames[j]}: {max_similarity:.2%}")

    def run_analysis(self, folder_path: str) -> Dict[str, np.ndarray]:
        """
        –ü–æ–ª–Ω—ã–π –∑–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–∞–≥–∏–∞—Ç–∞

        Args:
            folder_path: –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–∞—Ç—Ä–∏—Ü–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        print("üöÄ –ó–∞–ø—É—Å–∫ Educational Plagiarism Detector")
        print("=" * 60)

        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            self.load_documents(folder_path)

            if not self.documents:
                print("‚ùå –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ!")
                print(f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: .txt, .pdf")
                print(f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å: {folder_path}")
                return {}

            print(f"\nüìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            self.process_all_documents()

            # –†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏
            similarity_matrices = self.calculate_similarity_matrix()

            if similarity_matrices:
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                self.visualize_results(similarity_matrices)

                # –û—Ç—á–µ—Ç
                self.generate_report(similarity_matrices)

            return similarity_matrices

        except Exception as e:
            print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
            import traceback

            traceback.print_exc()
            return {}


def create_test_documents(folder_name: str = "test_documents"):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    import os

    os.makedirs(folder_name, exist_ok=True)

    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å NLTK)
    texts = {
        "–¥–æ–∫—É–º–µ–Ω—Ç1.txt": """–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (–ò–ò) - —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º—ã–π –º–∞—à–∏–Ω–∞–º–∏, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –ø—Ä–æ—è–≤–ª—è–µ–º–æ–≥–æ –ª—é–¥—å–º–∏ –∏ –∂–∏–≤–æ—Ç–Ω—ã–º–∏. –í–µ–¥—É—â–∏–µ —É—á–µ–±–Ω–∏–∫–∏ –ø–æ –ò–ò –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç —ç—Ç—É –æ–±–ª–∞—Å—Ç—å –∫–∞–∫ –∏–∑—É—á–µ–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤: –ª—é–±–æ–π —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä–∞—è –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—â–∏–µ –µ–µ —à–∞–Ω—Å—ã –Ω–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–µ–π.""",
        "–¥–æ–∫—É–º–µ–Ω—Ç2.txt": """–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –ø—Ä–æ—è–≤–ª—è–µ–º—ã–π –º–∞—à–∏–Ω–∞–º–∏, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º–æ–≥–æ –ª—é–¥—å–º–∏ –∏ –∂–∏–≤–æ—Ç–Ω—ã–º–∏. –û—Å–Ω–æ–≤–Ω—ã–µ —É—á–µ–±–Ω–∏–∫–∏ –ø–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç —ç—Ç—É –æ–±–ª–∞—Å—Ç—å –∫–∞–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤: —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—é—Ç –æ–∫—Ä—É–∂–∞—é—â—É—é —Å—Ä–µ–¥—É –∏ –¥–µ–π—Å—Ç–≤—É—é—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Å–≤–æ–∏—Ö —Ü–µ–ª–µ–π.""",
        "–¥–æ–∫—É–º–µ–Ω—Ç3.txt": """–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ —Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –±–µ–∑ —è–≤–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–Ω–∏ –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –∏ —É–º–æ–∑–∞–∫–ª—é—á–µ–Ω–∏—è.""",
        "–¥–æ–∫—É–º–µ–Ω—Ç4.txt": """–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞ - —ç—Ç–æ –∏–∑—É—á–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –º–∞—à–∏–Ω –∏ —Å–∞–º–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è. –ö–∞–∫ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π –∫—Ä—É–≥ —Ç–µ–º - –æ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤ –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–º –∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–∏.""",
    }

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    created_files = []
    for filename, content in texts.items():
        filepath = os.path.join(folder_name, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        created_files.append(filename)

    print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(created_files)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–ø–∫–µ '{folder_name}'")
    return folder_name


def create_test_documents_english(folder_name: str = "english_documents"):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ"""
    import os

    os.makedirs(folder_name, exist_ok=True)

    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ
    texts = {
        "document1.txt": """Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. Leading AI textbooks define the field as the study of intelligent agents: any system that perceives its environment and takes actions that maximize its chance of achieving its goals.""",
        "document2.txt": """Artificial intelligence is intelligence exhibited by machines, unlike the natural intelligence shown by humans and animals. Major AI textbooks describe this field as the research of intelligent agents: systems that perceive their surroundings and act to maximize the likelihood of accomplishing their objectives.""",
        "document3.txt": """Machine learning is a branch of artificial intelligence that focuses on the development of algorithms and statistical models that enable computer systems to perform tasks without explicit instructions. Instead, they rely on patterns and inference.""",
        "document4.txt": """Computer science is the study of algorithmic processes, computational machines, and computation itself. As a discipline, computer science spans a range of topics from theoretical studies of algorithms to the practical issues of implementing computational systems in hardware and software.""",
    }

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    created_files = []
    for filename, content in texts.items():
        filepath = os.path.join(folder_name, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        created_files.append(filename)

    print(f"‚úì Created {len(created_files)} test documents in '{folder_name}' folder")
    return folder_name


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("Educational Plagiarism Detector - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è")
    print("=" * 60)

    print("\n–í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    print("1. –†—É—Å—Å–∫–∏–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è - –º–µ–Ω—å—à–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)")
    print("2. –ê–Ω–≥–ª–∏–π—Å–∫–∏–π")

    choice = input("–í–≤–µ–¥–∏—Ç–µ 1 –∏–ª–∏ 2: ").strip()

    if choice == "1":
        print("\nüá∑üá∫ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫")
        test_folder = create_test_documents()
        detector = PlagiarismDetector(min_similarity_threshold=0.4, language="russian")
    else:
        print("\nüá¨üáß Using English language")
        test_folder = create_test_documents_english()
        detector = PlagiarismDetector(min_similarity_threshold=0.4, language="english")

    # –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    results = detector.run_analysis(test_folder)

    if results:
        print("\n" + "=" * 60)
        print("‚úÖ –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print("=" * 60)
